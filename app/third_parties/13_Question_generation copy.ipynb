{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question Generation\n",
    "\n",
    "This is a bare bones tutorial showing what is possible with the QuestionGenerator Nodes and Pipelines which automatically\n",
    "generate questions which the question generation model thinks can be answered by a given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yaaKv3_ZN-gb"
   },
   "source": [
    "\n",
    "## Preparing the Colab Environment\n",
    "\n",
    "- [Enable GPU Runtime](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration#enabling-the-gpu-in-colab)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Haystack\n",
    "\n",
    "To start, let's install the latest release of Haystack with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (24.3.1)\n",
      "Requirement already satisfied: farm-haystack[colab,elasticsearch,inference] in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: boilerpy3 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (1.0.7)\n",
      "Requirement already satisfied: events in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (0.5)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (0.27.2)\n",
      "Requirement already satisfied: jsonschema in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (4.23.0)\n",
      "Requirement already satisfied: lazy-imports==0.3.1 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (0.3.1)\n",
      "Requirement already satisfied: more-itertools in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (10.5.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (3.2.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (2.2.3)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (11.0.0)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (4.3.6)\n",
      "Requirement already satisfied: posthog in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (3.7.2)\n",
      "Requirement already satisfied: prompthub-py==4.0.0 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (4.0.0)\n",
      "Requirement already satisfied: pydantic<2 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (1.10.19)\n",
      "Requirement already satisfied: quantulum3 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (0.9.2)\n",
      "Requirement already satisfied: rank-bm25 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (0.2.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (2.32.3)\n",
      "Requirement already satisfied: requests-cache<1.0.0 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (0.9.8)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (1.5.2)\n",
      "Requirement already satisfied: sseclient-py in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (1.8.0)\n",
      "Requirement already satisfied: tenacity in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (0.8.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (4.67.0)\n",
      "Requirement already satisfied: transformers<5.0,>=4.46 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (4.46.3)\n",
      "Collecting elastic-transport<8 (from farm-haystack[colab,elasticsearch,inference])\n",
      "  Using cached elastic_transport-7.16.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting elasticsearch<8,>=7.17 (from farm-haystack[colab,elasticsearch,inference])\n",
      "  Using cached elasticsearch-7.17.12-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting pillow (from farm-haystack[colab,elasticsearch,inference])\n",
      "  Downloading Pillow-9.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.5.0 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from farm-haystack[colab,elasticsearch,inference]) (0.26.2)\n",
      "Collecting sentence-transformers<=3.0.0,>=2.3.1 (from farm-haystack[colab,elasticsearch,inference])\n",
      "  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=6.0 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from prompthub-py==4.0.0->farm-haystack[colab,elasticsearch,inference]) (6.0.2)\n",
      "Collecting urllib3<2,>=1.21.1 (from elastic-transport<8->farm-haystack[colab,elasticsearch,inference])\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: six>=1.12 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from elastic-transport<8->farm-haystack[colab,elasticsearch,inference]) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from elastic-transport<8->farm-haystack[colab,elasticsearch,inference]) (2024.8.30)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from huggingface-hub>=0.5.0->farm-haystack[colab,elasticsearch,inference]) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from huggingface-hub>=0.5.0->farm-haystack[colab,elasticsearch,inference]) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from huggingface-hub>=0.5.0->farm-haystack[colab,elasticsearch,inference]) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from huggingface-hub>=0.5.0->farm-haystack[colab,elasticsearch,inference]) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from requests->farm-haystack[colab,elasticsearch,inference]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from requests->farm-haystack[colab,elasticsearch,inference]) (3.10)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference]) (1.4.4)\n",
      "Requirement already satisfied: attrs>=21.2 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference]) (24.2.0)\n",
      "Requirement already satisfied: cattrs>=22.2 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference]) (24.1.2)\n",
      "Requirement already satisfied: url-normalize>=1.4 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference]) (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from scikit-learn>=1.3.0->farm-haystack[colab,elasticsearch,inference]) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from scikit-learn>=1.3.0->farm-haystack[colab,elasticsearch,inference]) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from scikit-learn>=1.3.0->farm-haystack[colab,elasticsearch,inference]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from scikit-learn>=1.3.0->farm-haystack[colab,elasticsearch,inference]) (3.5.0)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers<=3.0.0,>=2.3.1->farm-haystack[colab,elasticsearch,inference])\n",
      "  Downloading torch-2.5.1-cp39-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from tiktoken>=0.5.1->farm-haystack[colab,elasticsearch,inference]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from transformers<5.0,>=4.46->farm-haystack[colab,elasticsearch,inference]) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from transformers<5.0,>=4.46->farm-haystack[colab,elasticsearch,inference]) (0.4.5)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece,torch]<5.0,>=4.46; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference])\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting protobuf (from transformers[sentencepiece,torch]<5.0,>=4.46; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference])\n",
      "  Using cached protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting accelerate>=0.26.0 (from transformers[sentencepiece,torch]<5.0,>=4.46; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference])\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from httpx->farm-haystack[colab,elasticsearch,inference]) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from httpx->farm-haystack[colab,elasticsearch,inference]) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from httpx->farm-haystack[colab,elasticsearch,inference]) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from httpcore==1.*->httpx->farm-haystack[colab,elasticsearch,inference]) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from jsonschema->farm-haystack[colab,elasticsearch,inference]) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from jsonschema->farm-haystack[colab,elasticsearch,inference]) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from jsonschema->farm-haystack[colab,elasticsearch,inference]) (0.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from pandas->farm-haystack[colab,elasticsearch,inference]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from pandas->farm-haystack[colab,elasticsearch,inference]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from pandas->farm-haystack[colab,elasticsearch,inference]) (2024.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from posthog->farm-haystack[colab,elasticsearch,inference]) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from posthog->farm-haystack[colab,elasticsearch,inference]) (2.2.1)\n",
      "Requirement already satisfied: inflect in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from quantulum3->farm-haystack[colab,elasticsearch,inference]) (7.4.0)\n",
      "Requirement already satisfied: num2words in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from quantulum3->farm-haystack[colab,elasticsearch,inference]) (0.5.13)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.46; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference]) (5.8.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.1.1 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from cattrs>=22.2->requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference]) (1.2.2)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers<=3.0.0,>=2.3.1->farm-haystack[colab,elasticsearch,inference])\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers<=3.0.0,>=2.3.1->farm-haystack[colab,elasticsearch,inference])\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers<=3.0.0,>=2.3.1->farm-haystack[colab,elasticsearch,inference])\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from inflect->quantulum3->farm-haystack[colab,elasticsearch,inference]) (4.4.1)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from num2words->quantulum3->farm-haystack[colab,elasticsearch,inference]) (0.6.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from typeguard>=4.0.1->inflect->quantulum3->farm-haystack[colab,elasticsearch,inference]) (8.5.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers<=3.0.0,>=2.3.1->farm-haystack[colab,elasticsearch,inference])\n",
      "  Using cached MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/haystack/lib/python3.9/site-packages (from importlib-metadata>=3.6->typeguard>=4.0.1->inflect->quantulum3->farm-haystack[colab,elasticsearch,inference]) (3.21.0)\n",
      "Downloading elastic_transport-7.16.0-py2.py3-none-any.whl (35 kB)\n",
      "Downloading elasticsearch-7.17.12-py2.py3-none-any.whl (385 kB)\n",
      "Downloading Pillow-9.0.0-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m240.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n",
      "Downloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "Downloading sentencepiece-0.2.0-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m168.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp39-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m233.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m273.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Using cached protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m767.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, mpmath, urllib3, sympy, protobuf, pillow, MarkupSafe, jinja2, elasticsearch, elastic-transport, torch, accelerate, sentence-transformers\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.0.0\n",
      "    Uninstalling pillow-11.0.0:\n",
      "      Successfully uninstalled pillow-11.0.0\n",
      "Successfully installed MarkupSafe-3.0.2 accelerate-1.1.1 elastic-transport-7.16.0 elasticsearch-7.17.12 jinja2-3.1.4 mpmath-1.3.0 pillow-9.0.0 protobuf-5.28.3 sentence-transformers-3.0.0 sentencepiece-0.2.0 sympy-1.13.1 torch-2.5.1 urllib3-1.26.20\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install farm-haystack[colab,elasticsearch,inference]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling Telemetry \n",
    "Knowing you're using this tutorial helps us decide where to invest our efforts to build a better product but you can always opt out by commenting the following line. See [Telemetry](https://docs.haystack.deepset.ai/docs/telemetry) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.telemetry import tutorial_running\n",
    "\n",
    "tutorial_running(13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Logging\n",
    "\n",
    "We configure how logging messages should be displayed and which log level should be used before importing Haystack.\n",
    "Example log message:\n",
    "INFO - haystack.utils.preprocessing -  Converting data/tutorial1/218_Olenna_Tyrell.txt\n",
    "Default log level in basicConfig is WARNING so the explicit parameter is not necessary but can be changed easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports needed to run this notebook\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "from haystack.nodes import QuestionGenerator, BM25Retriever, FARMReader\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.pipelines import (\n",
    "    QuestionGenerationPipeline,\n",
    "    RetrieverQuestionGenerationPipeline,\n",
    "    QuestionAnswerGenerationPipeline,\n",
    ")\n",
    "from haystack.utils import launch_es, print_questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's start an Elasticsearch instance with one of the options below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to find image 'elasticsearch:7.17.6' locally\n",
      "7.17.6: Pulling from library/elasticsearch\n",
      "57ed440d6d89: Pulling fs layer\n",
      "a749a280e3e9: Pulling fs layer\n",
      "72050ffc35ca: Pulling fs layer\n",
      "47e52584ee03: Pulling fs layer\n",
      "73b52868e110: Pulling fs layer\n",
      "d51844c6f6ab: Pulling fs layer\n",
      "eacd8a3b0316: Pulling fs layer\n",
      "a666a9c77b42: Pulling fs layer\n",
      "3dfebdff3d8d: Pulling fs layer\n",
      "3dfebdff3d8d: Download complete\n",
      "47e52584ee03: Download complete\n",
      "eacd8a3b0316: Download complete\n",
      "d51844c6f6ab: Download complete\n",
      "57ed440d6d89: Download complete\n",
      "a666a9c77b42: Download complete\n",
      "a749a280e3e9: Download complete\n",
      "72050ffc35ca: Download complete\n",
      "73b52868e110: Download complete\n",
      "Digest: sha256:6c128de5d01c0c130a806022d6bd99b3e4c27a9af5bfc33b6b81861ae117d028\n",
      "Status: Downloaded newer image for elasticsearch:7.17.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6d49a2c3819ccb05cef09060ef28ff6e39854c7a2764d1c9c1fe299b0b1544a1\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Start Elasticsearch service via Docker\n",
    "launch_es()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's initialize some core components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "INFO - haystack.modeling.utils -  Using devices: MPS - Number of GPUs: 1\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/thuhoaitranthi/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\"\n",
    "text2 = \"Princess Arya Stark is the third child and second daughter of Lord Eddard Stark and his wife, Lady Catelyn Stark. She is the sister of the incumbent Westerosi monarchs, Sansa, Queen in the North, and Brandon, King of the Andals and the First Men. After narrowly escaping the persecution of House Stark by House Lannister, Arya is trained as a Faceless Man at the House of Black and White in Braavos, using her abilities to avenge her family. Upon her return to Westeros, she exacts retribution for the Red Wedding by exterminating the Frey male line.\"\n",
    "text3 = \"Dry Cleaning are an English post-punk band who formed in South London in 2018.[3] The band is composed of vocalist Florence Shaw, guitarist Tom Dowse, bassist Lewis Maynard and drummer Nick Buxton. They are noted for their use of spoken word primarily in lieu of sung vocals, as well as their unconventional lyrics. Their musical stylings have been compared to Wire, Magazine and Joy Division.[4] The band released their debut single, 'Magic of Meghan' in 2019. Shaw wrote the song after going through a break-up and moving out of her former partner's apartment the same day that Meghan Markle and Prince Harry announced they were engaged.[5] This was followed by the release of two EPs that year: Sweet Princess in August and Boundary Road Snacks and Drinks in October. The band were included as part of the NME 100 of 2020,[6] as well as DIY magazine's Class of 2020.[7] The band signed to 4AD in late 2020 and shared a new single, 'Scratchcard Lanyard'.[8] In February 2021, the band shared details of their debut studio album, New Long Leg. They also shared the single 'Strong Feelings'.[9] The album, which was produced by John Parish, was released on 2 April 2021.[10]\"\n",
    "\n",
    "docs = [{\"content\": text1}, {\"content\": text2}, {\"content\": text3}]\n",
    "\n",
    "# Initialize document store and write in the documents\n",
    "document_store = ElasticsearchDocumentStore()\n",
    "document_store.write_documents(docs)\n",
    "\n",
    "# Initialize Question Generator\n",
    "question_generator = QuestionGenerator()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question Generation Pipeline\n",
    "\n",
    "The most basic version of a question generator pipeline takes a document as input and outputs generated questions\n",
    "which the the document can answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * Generating questions for document 0: Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Ros...\n",
      "\n",
      "\n",
      "Generated questions:\n",
      " - Who created Python?\n",
      " - When was Python first released?\n",
      " - What is Python's design philosophy?\n",
      "\n",
      " * Generating questions for document 1: Princess Arya Stark is the third child and second daughter of Lord Eddard Stark and his wife, Lady C...\n",
      "\n",
      "\n",
      "Generated questions:\n",
      " - Who is the third child and second daughter of Lord Eddard Stark and his wife, Lady Catelyn Stark?\n",
      " - Princess Arya Stark is the sister of what Westerosi monarchs?\n",
      " - What is Sansa, Queen in the North, and Brandon, King of the Andals?\n",
      " - What is Arya trained as?\n",
      " - Where is the House of Black and White located?\n",
      " - What is the name of the first men?\n",
      " - What is the name of the line that Frey exterminates?\n",
      " - Where does the Red Wedding take place?\n",
      "\n",
      " * Generating questions for document 2: Dry Cleaning are an English post-punk band who formed in South London in 2018.[3] The band is compos...\n",
      "\n",
      "\n",
      "Generated questions:\n",
      " - What is the name of the English post-punk band that formed in South London in 2018?\n",
      " - Who is the vocalist of Dry Cleaning?\n",
      " - Where did Dry Cleaning form?\n",
      " - What does the band use instead of sung vocals?\n",
      " - What is the name of the band's debut single?\n",
      " - What was Shaw's first song called?\n",
      " - When did Shaw write the song Magic of Meghan?\n",
      " - When did Meghan Markle and Prince Harry announce they were engaged?\n",
      " - What was the name of the two EPs released in August?\n",
      " - When was Boundary Road Snacks and Drinks released?\n",
      " - When did Boundary Road Snacks and Drinks release their album?\n",
      " - When did the band sign to 4AD?\n",
      " - What was the name of the new single that the band shared in February 2021?\n",
      " - Which magazine included the band in the Class of 2020?\n",
      " - What was the name of the new single released in February 2021?\n",
      " - Who produced the album New Long Leg?\n",
      " - When was the album released?\n"
     ]
    }
   ],
   "source": [
    "question_generation_pipeline = QuestionGenerationPipeline(question_generator)\n",
    "for idx, document in enumerate(document_store):\n",
    "\n",
    "    print(f\"\\n * Generating questions for document {idx}: {document.content[:100]}...\\n\")\n",
    "    result = question_generation_pipeline.run(documents=[document])\n",
    "    print_questions(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Retriever Question Generation Pipeline\n",
    "\n",
    "This pipeline takes a query as input. It retrieves relevant documents and then generates questions based on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * Generating questions for documents matching the query 'Arya Stark'\n",
      "\n",
      "\n",
      "Generated questions:\n",
      " - Who is the third child and second daughter of Lord Eddard Stark and his wife, Lady Catelyn Stark?\n",
      " - Princess Arya Stark is the sister of what Westerosi monarchs?\n",
      " - What is Sansa, Queen in the North, and Brandon, King of the Andals?\n",
      " - What is Arya trained as?\n",
      " - Where is the House of Black and White located?\n",
      " - What is the name of the first men?\n",
      " - What is the name of the line that Frey exterminates?\n",
      " - Where does the Red Wedding take place?\n"
     ]
    }
   ],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)\n",
    "rqg_pipeline = RetrieverQuestionGenerationPipeline(retriever, question_generator)\n",
    "\n",
    "print(f\"\\n * Generating questions for documents matching the query 'Arya Stark'\\n\")\n",
    "result = rqg_pipeline.run(query=\"Arya Stark\")\n",
    "print_questions(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question Answer Generation Pipeline\n",
    "\n",
    "This pipeline takes a document as input, generates questions on it, and attempts to answer these questions using\n",
    "a Reader model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: MPS - Number of GPUs: 1\n",
      "INFO - haystack.modeling.utils -  Using devices: MPS - Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -   * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)\n",
      "INFO - haystack.modeling.model.language_model -  Auto-detected model language: english\n",
      "INFO - haystack.modeling.model.language_model -  Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.\n",
      "INFO - haystack.modeling.utils -  Using devices: MPS - Number of GPUs: 1\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * Generating questions and answers for document 0: Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Ros...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples:   0%|          | 0/1 [00:02<?, ? Batches/s]\n",
      "0it [00:03, ?it/s]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Exception while running node 'Reader': The operator 'aten::tril_indices' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\nEnable debug logging to see the data that was passed when the pipeline failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/pipelines/base.py:567\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, query, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[1;32m    566\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m--> 567\u001b[0m node_output, stream_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m node_output \u001b[38;5;129;01mand\u001b[39;00m node_id \u001b[38;5;129;01min\u001b[39;00m node_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/pipelines/base.py:469\u001b[0m, in \u001b[0;36mPipeline._run_node\u001b[0;34m(self, node_id, node_input)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_id: \u001b[38;5;28mstr\u001b[39m, node_input: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnode_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/nodes/base.py:201\u001b[0m, in \u001b[0;36mBaseComponent._dispatch_run\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mThe Pipelines call this method when run() is executed. This method in turn executes the _dispatch_run_general()\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03mmethod with the correct run method.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_run_general\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/nodes/base.py:245\u001b[0m, in \u001b[0;36mBaseComponent._dispatch_run_general\u001b[0;34m(self, run_method, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m         run_inputs[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 245\u001b[0m output, stream \u001b[38;5;241m=\u001b[39m \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Collect debug information\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/nodes/reader/base.py:181\u001b[0m, in \u001b[0;36mBaseReader.run_batch\u001b[0;34m(self, queries, documents, top_k, batch_size, labels, add_isolated_node_eval)\u001b[0m\n\u001b[1;32m    179\u001b[0m predict_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtiming(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_batch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Add corresponding document_name and more meta data, if an answer contains the document_id\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/nodes/reader/base.py:241\u001b[0m, in \u001b[0;36mBaseReader.timing.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m tic \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m--> 241\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m toc \u001b[38;5;241m=\u001b[39m perf_counter()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/nodes/reader/farm.py:869\u001b[0m, in \u001b[0;36mFARMReader.predict_batch\u001b[0;34m(self, queries, documents, top_k, batch_size)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_batch \u001b[38;5;129;01min\u001b[39;00m get_batches_from_generator(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing_batch_size):\n\u001b[0;32m--> 869\u001b[0m     cur_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minferencer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_from_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiprocessing_chunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mextend(cur_predictions)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/modeling/infer.py:518\u001b[0m, in \u001b[0;36mQAInferencer.inference_from_objects\u001b[0;34m(self, objects, return_json, multiprocessing_chunksize)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# TODO investigate this deprecation warning. Timo: I thought we were about to implement Input Objects,\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# then we can and should use inference from (input) objects!\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# logger.warning(\"QAInferencer.inference_from_objects() will soon be deprecated. Use QAInferencer.inference_from_dicts() instead\")\u001b[39;00m\n\u001b[0;32m--> 518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_from_dicts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiprocessing_chunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocessing_chunksize\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/modeling/infer.py:482\u001b[0m, in \u001b[0;36mQAInferencer.inference_from_dicts\u001b[0;34m(self, dicts, return_json, multiprocessing_chunksize)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m:param multiprocessing_chunksize: number of dicts to put together in one chunk and feed to one process\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;124;03m                                  (only relevant if you do multiprocessing)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m                            has been deprecated.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInferencer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_from_dicts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiprocessing_chunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocessing_chunksize\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/modeling/infer.py:315\u001b[0m, in \u001b[0;36mInferencer.inference_from_dicts\u001b[0;34m(self, dicts, return_json, multiprocessing_chunksize)\u001b[0m\n\u001b[1;32m    313\u001b[0m     aggregate_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mprediction_heads[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregate_preds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 315\u001b[0m predictions: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_without_multiprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/modeling/infer.py:337\u001b[0m, in \u001b[0;36mInferencer._inference_without_multiprocessing\u001b[0;34m(self, dicts, return_json, aggregate_preds)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggregate_preds:\n\u001b[0;32m--> 337\u001b[0m     preds_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_predictions_and_aggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaskets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/modeling/infer.py:417\u001b[0m, in \u001b[0;36mInferencer._get_predictions_and_aggregate\u001b[0;34m(self, dataset, tensor_names, baskets)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# preds = self.model.logits_to_preds(logits, **batch)[0] (This must somehow be useful for SQuAD)\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_to_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m unaggregated_preds_all\u001b[38;5;241m.\u001b[39mappend(preds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/modeling/model/adaptive_model.py:73\u001b[0m, in \u001b[0;36mBaseAdaptiveModel.logits_to_preds\u001b[0;34m(self, logits, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m head, logits_for_head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_heads, logits):\n\u001b[0;32m---> 73\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mhead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_to_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_for_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     all_preds\u001b[38;5;241m.\u001b[39mappend(preds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/modeling/model/prediction_head.py:471\u001b[0m, in \u001b[0;36mQuestionAnsweringHead.logits_to_preds\u001b[0;34m(self, logits, span_mask, start_of_word, seq_2_start_t, max_answer_length, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# disqualify answers where end < start\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# (set the lower triangular matrix to low value, excluding diagonal)\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtril_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_end_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m start_end_matrix[:, indices[\u001b[38;5;241m0\u001b[39m][:], indices[\u001b[38;5;241m1\u001b[39m][:]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m888\u001b[39m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::tril_indices' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, document \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(document_store)):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m * Generating questions and answers for document \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocument\u001b[38;5;241m.\u001b[39mcontent[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mqag_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     print_questions(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/pipelines/standard_pipelines.py:671\u001b[0m, in \u001b[0;36mQuestionAnswerGenerationPipeline.run\u001b[0;34m(self, documents, params, debug)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28mself\u001b[39m, documents: List[Document], params: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, debug: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    670\u001b[0m ):\n\u001b[0;32m--> 671\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.9/site-packages/haystack/pipelines/base.py:574\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, query, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# The input might be a really large object with thousands of embeddings.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# If you really want to see it, raise the log level.\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while running node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with input \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, node_id, node_input)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while running node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnable debug logging to see the data that was passed when the pipeline failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    577\u001b[0m queue\u001b[38;5;241m.\u001b[39mpop(node_id)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Exception while running node 'Reader': The operator 'aten::tril_indices' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\nEnable debug logging to see the data that was passed when the pipeline failed."
     ]
    }
   ],
   "source": [
    "reader = FARMReader(\"deepset/roberta-base-squad2\")\n",
    "qag_pipeline = QuestionAnswerGenerationPipeline(question_generator, reader)\n",
    "for idx, document in enumerate(tqdm(document_store)):\n",
    "\n",
    "    print(f\"\\n * Generating questions and answers for document {idx}: {document.content[:100]}...\\n\")\n",
    "    result = qag_pipeline.run(documents=[document])\n",
    "    print_questions(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Translated Question Answer Generation Pipeline\n",
    "Trained models for Question Answer Generation are not available in many languages other than English. Haystack\n",
    "provides a workaround for that issue by machine-translating a pipeline's inputs and outputs with the\n",
    "TranslationWrapperPipeline. The following example generates German questions and answers on a German text\n",
    "document - by using an English model for Question Answer Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill the document store with a German document.\n",
    "text1 = \"Python ist eine interpretierte Hochsprachenprogrammiersprache für allgemeine Zwecke. Sie wurde von Guido van Rossum entwickelt und 1991 erstmals veröffentlicht. Die Design-Philosophie von Python legt den Schwerpunkt auf die Lesbarkeit des Codes und die Verwendung von viel Leerraum (Whitespace).\"\n",
    "docs = [{\"content\": text1}]\n",
    "document_store.delete_documents()\n",
    "document_store.write_documents(docs)\n",
    "\n",
    "# Load machine translation models\n",
    "from haystack.nodes import TransformersTranslator\n",
    "\n",
    "in_translator = TransformersTranslator(model_name_or_path=\"Helsinki-NLP/opus-mt-de-en\")\n",
    "out_translator = TransformersTranslator(model_name_or_path=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "\n",
    "# Wrap the previously defined QuestionAnswerGenerationPipeline\n",
    "from haystack.pipelines import TranslationWrapperPipeline\n",
    "\n",
    "pipeline_with_translation = TranslationWrapperPipeline(\n",
    "    input_translator=in_translator, output_translator=out_translator, pipeline=qag_pipeline\n",
    ")\n",
    "\n",
    "for idx, document in enumerate(tqdm(document_store)):\n",
    "    print(f\"\\n * Generating questions and answers for document {idx}: {document.content[:100]}...\\n\")\n",
    "    result = pipeline_with_translation.run(documents=[document])\n",
    "    print_questions(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
